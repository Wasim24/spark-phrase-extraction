
# Spark Phrase Extraction : Automated phrase mining from huge text corpus using Apache Spark

This library is similar to phrase extraction implementation in Gensim([found here](https://github.com/RaRe-Technologies/gensim)) but for huge text corpus at scale using apache Spark. 

[![Build Status](https://travis-ci.org/spoddutur/spark-phrase-extraction.svg?branch=master)](https://travis-ci.org/spoddutur/spark-phrase-extraction)
[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/spark-phrase-extraction?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=body_badge)
[![codecov](https://codecov.io/gh/spoddutur/spark-phrase-extraction/branch/master/graph/badge.svg)](https://codecov.io/gh/spoddutur/spark-phrase-extraction)

**Target audience:** Spark-Scala ML applications in the need of collocations phrase detection for their natural language processing (NLP) and information retrieval (IR) tasks.
<br/>

## spark-parse-extraction provides:
- Basic building blocks to create ML applications utilizing GenSim API's to:
  - To Train a distributed collocation-based corpus vocabulary
  - To Save the trained model
  - To Load the saved model and use it with its corpus knowledge to predict collocated n-gram phrases in input sentences.
  - Scoring:
    - Supports default python-gensim scorers: Original Scoring and NPMI Scoring
    - Enabled config-based-approach to plugin and play custom scorers
    - Added Contingency-Based scorers for use with Phraser like ChiSq, Jaccard, LogLikelyHood etc

## How to Run
1. Init a set of config params. Following are the params supported:
  - `minCount:` minimum number of times a ngram should appear to get added into corpus.
  - `threshold:` threshold on score generated by scorer. Its used to prune vocab, when it learns more than maxVocabSize.
  - `maxVocabSize:` maximun default of 4,00,000 corpus is learnt
  - `delimiter:` used to concat ngrams identified and replace in a sentence. Example Sentence: `San Francisco is a beautiful city`. After ngrams identification: `San_Francisco is a beautiful_city`. Tokens are concatenated with default "_" delimiter.
  - `progressPer:` While training, how frequently to show progress
  - `commonWords:` Used to concate common words occuring in between one gram's ex: `bank_of_america` is identified as a bigram provided `of` is set as a common word.
  
Initialise them as:
```markdown
val common_words = mutable.HashSet[String]("of", "with", "without", "and", "or", "the", "a")
    val config: PhrasesConfig = new SimplePhrasesConfig().copy(minCount = 1, threshold = 1.0f, commonWords = Some(common_words))
    val configBc = spark.sparkContext.broadcast(config)
```

2. Init Scorer
Fetch any of DEFAULT, NPMI, JACCARD, CHI_SQ bigram scorers or add your custom scorer.
```markdown
    val scorer = BigramScorer.getScorer(BigramScorer.DEFAULT)
```

3. Start training
```markdown
  CorpusHolder.learnAndSave(spark, sentencesDf, configBc, scorer, outputPath)
```
**Params:**
  - sentencesDf -> input dataframe of sentences that spark reads
  - configBc -> PhraserConfig which we initialised in step1
  - scorer -> initialised in step2
  - outputPath -> where to save the model
  
4. Start Predicting
```markdown
    // load model from output path in step3 above
    val phrases = Util.load[Phrases]("/tmp/gensim-model")
    val phraserBc = spark.sparkContext.broadcast(new Phraser(phrases))
    
    // predict
    val sentenceBigramsDf = sentencesDf
                  .map(sentence => phraserBc.value.apply(sentence.split(" ")))

    // write bigrams to file
    sentenceBigramsDf.write.json("/tmp/gensim-output/")
 ```
 
 Please refer to [Predictor implementation](https://github.com/spoddutur/spark-phrase-extraction/blob/master/src/main/scala/SparkPhrasePredictor.scala) and [Trainer implementation](https://github.com/spoddutur/spark-phrase-extraction/blob/master/src/main/scala/ClusterPhraseExtractionTrainer.scala) where I've put together training and prediction as a working example.
 
## How to train for Trigrams?
- Above step trains bigrams.
- If we want trigrams, repeat step3 for `sentenceBigramsDf` as shown below
```markdown
  CorpusHolder.learnAndSave(spark, sentenceBigramsDf, configBc, scorer, outputPath)
```
- Similarly repeat the step for each higher n-gram we want.

## References
- Heavily inspired from the good work that Python-Gensim has done [here](https://radimrehurek.com/gensim/models/phrases.html) and [here](http://pydoc.net/gensim/3.2.0/gensim.models.phrases/)
- [Python-Gensim Github](https://github.com/RaRe-Technologies/gensim)
- [Python-Gensim.ipynb on How to use it](https://github.com/jdwittenauer/ipython-notebooks/blob/master/notebooks/libraries/Gensim.ipynb)
- [Custom Scoring support via contingency-based scoring for collocations and statistical analysis of n-grams](http://dspace.uib.no/bitstream/handle/1956/11033/lyse-andersen-mwe-final.pdf?sequence=1&isAllowed=y)
